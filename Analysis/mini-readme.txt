The pipeline requires the following python modules to be installed:

- numpy
- egg
- torch
- spacy

Roberto should send an interaction file with (minimally) the following fields:

 m.aux_input["decoded_messages"] # neural captions
 m.aux_input["decoded_captions"] # human captions

An example of such file from the ImageCode experiments is: clipclap_coco_transf_frozen_gpt.interaction

From an interaction file like this, we use the generate_tab_delimited_file.py to create a tab-delimited file that simply contains "dummy" neural/human accuracy and is_video columns, followed by neural and human captions. The reason to have the dummy fields is that they are expected by the following scripts, although they are irrelevant for our current analysis.

The generate_tab_delimited_file.py takes as arguments the path to the input interaction file and a name to assign to the output text file to be generated, like this (pippo.interaction is a copy of clipclap_coco_transf_frozen_gpt.interaction included in this directory for you to experiment with the pipeline):

% python generate_tab_delimited_file.py pippo.interaction pippo.txt

We now feed the generated tab-delimited text file to the pre-process-sentences-and-get-basic-stats.py script, in order to obtain files that contain lemma and pos counts in the neural and human caption sets, respectively, as well as sending some basic stats to STDOUT.

The pre-process-sentences-and-get-basic-stats.py script takes the name of the tab delimited file (without the .txt extension) as input, and generates two files with the same name (with .txt extension) prefixed by pos_ and lemma_, respectively.

For example, given the pippo.txt file above, you run the script as follows:

% python pre-process-sentences-and-get-basic-stats.py pippo
total captions: 2302
model sent length: 11.341876629018245 (2.4293620108457494)
human sent length: 22.5251954821894 (8.984058636487992)
IOU: 0.06371829151166272 (0.07810885505651442)

Now, we got two output files pos_pippo.txt and lemma_pippo.txt. Each of them has 3 tab-delimited fields with a lemma or POS (first col) and the corresponding count in the neural (second col) and human (third col) caption corpora, respectively.

For example, given the input above, pos_pippo.txt contains the following lines:

% cat pos_pippo.txt 
PART	47	974
NOUN	8303	12371
ADJ	831	4622
PUNCT	2339	3614
PRON	394	2977
AUX	338	3552
ADP	5087	6916
NUM	60	516
SCONJ	8	223
VERB	2139	4055
INTJ	0	3
PROPN	100	263
CCONJ	214	1340
SYM	0	16
ADV	403	2204
DET	5846	8207

Francesca: these are the files I shared with you, to generate the frequency plots!

Now, in order to compute LMI, we need the overall size of the two corpora, which in my case I generate simply by summing the second (neural) and third (human) columns in one of the input files (in this case, the lemma file):

% perl -ane '{$tot1+=$F[1];$tot2+=$F[2]}END{print "$tot1 $tot2\n"}' lemma_pippo.txt
26109 51853

To compute LMI you feed the lmi.py script the two sizes (first neural, then human) and the name of a count file generated by pre-process-sentences-and-get-basic-stats.py (lemma_* or pos_*) as in:

% python lmi.py 26109 51853 pos_pippo.txt
% python lmi.py 26109 51853 lemma_pippo.txt

The lmi.py script generates an output file with same name as its input, but with lmi_ prefixed, e.g., from pos_pippo.txt, it generates lmi_pos_pippo.txt.

The tab-delimited fields pos/lemma, count in neural, LMI in neural, count in human, LMI in human.

For example, this is how the lmi_pos_pippo.txt file I generated above looks like:

% cat lmi_pos_pippo.txt
PART	47	-93.26909221059368	974	351.3044839930577
NOUN	8303	1508.5012383528035	12371	-1307.7769083625303
ADJ	831	-654.2878118190135	4622	1120.6978753783824
PUNCT	2339	373.7011866670712	3614	-329.85471571317436
PRON	394	-414.752909808355	2977	844.0252924004546
AUX	338	-456.021858148204	3552	1125.6657542282815
ADP	5087	1197.8516256876171	6916	-992.5173346756785
NUM	60	-70.0692991474408	516	153.66884599035922
SCONJ	8	-18.152277782604116	223	83.08150377989097
VERB	2139	65.66472157603138	4055	-64.15682198453561
INTJ	0	-0.0	3	1.223426208842744
PROPN	100	-19.529120226031615	263	22.502262692976768
CCONJ	214	-190.17539495631002	1340	347.9257717196973
SYM	0	-0.0	16	6.524939780494634
ADV	403	-311.55019460606235	2204	528.7010137349567
DET	5846	1267.7823641669524	8207	-1067.2359097372869

So, by sorting in decreasing order by field 3 or 5, I'll get the POSs or lemmas that are more typical of the neural or human corpus (note that I am illustrating this here with POSs, since it's easier to fit them into the readme, but in the paper we actually only used the lemma analysis, as the POS distribution can be easily eyeballed as a whole in the frequency spectrum generated by Francesca.
