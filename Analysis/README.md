The pipeline requires the following python modules to be installed:

- numpy
- egg (you can follow instructions here https://github.com/facebookresearch/EGG)
- torch
- spacy

An intereaction file is necessary to run the analysis.
Such file is the default format generated by the EGG library (https://github.com/facebookresearch/EGG).

The interaction is a dictionary-like object and the following fields should be defined to run the analysis
 m.aux_input["decoded_messages"] # neural captions
 m.aux_input["decoded_captions"] # human captions

An example of such file from the ImageCode experiments, and the one used for the paper is the file store in this folder and named clipclap_coco_transf_frozen_gpt.interaction.

From an interaction file like this, we use the generate_tab_delimited_file.py to create a tab-delimited file that simply contains "dummy" neural/human accuracy and is_video columns, followed by neural and human captions. The reason to have the dummy fields is that they are expected by the following scripts, although they are irrelevant for our current analysis. The is_video value is 1 if the target is in the video split of ImageCode.

The generate_tab_delimited_file.py takes as arguments the path to the input interaction file and a name to assign to the output text file to be generated, like this:

% python generate_tab_delimited_file.py clipcap_coco_transf_frozen_gpt.interaction clipcap_imagecode.txt

We now feed the generated tab-delimited text file to the pre-process-sentences-and-get-basic-stats.py script, in order to obtain files that contain lemma and pos counts in the neural and human caption sets, respectively, as well as sending some basic stats to STDOUT.

The pre-process-sentences-and-get-basic-stats.py script takes the name of the tab delimited file (without the .txt extension) as input, and generates two files with the same name (with .txt extension) prefixed by pos_ and lemma_, respectively.

For example, given the clipcap_imagecode.txt file above, you run the script as follows:

% python pre-process-sentences-and-get-basic-stats.py clipcap_imagecode
total captions: 2302
model sent length: 11.341876629018245 (2.4293620108457494)
human sent length: 22.5251954821894 (8.984058636487992)
IOU: 0.06371829151166272 (0.07810885505651442)

Now, we got two output files pos_pippo.txt and lemma_pippo.txt. Each of them has 3 tab-delimited fields with a lemma or POS (first col) and the corresponding count in the neural (second col) and human (third col) caption corpora, respectively.

For example, given the input above, clipcap_imagecode.txt contains the following lines (note that due to the randomness of set objects the order you see might be different):

% cat pos_clipcap_imagecode.txt 
PART	47	974
NOUN	8303	12371
ADJ	831	4622
PUNCT	2339	3614
PRON	394	2977
AUX	338	3552
ADP	5087	6916
NUM	60	516
SCONJ	8	223
VERB	2139	4055
INTJ	0	3
PROPN	100	263
CCONJ	214	1340
SYM	0	16
ADV	403	2204
DET	5846	8207

Now, in order to compute LMI values, we need the overall size of the two corpora.
You can obtain it simply by summing the second (neural) and third (human) columns in one of the input files (in this case, the lemma file):

% perl -ane '{$tot1+=$F[1];$tot2+=$F[2]}END{print "$tot1 $tot2\n"}' lemma_clipcap_imagecode.txt
26109 51853

To compute LMI you feed the lmi.py script the two sizes (first neural, then human) and the name of a count file generated by pre-process-sentences-and-get-basic-stats.py (lemma_* or pos_*) as in:

% python lmi.py 26109 51853 pos_clipcap_imagecode.txt
% python lmi.py 26109 51853 lemma_clipcap_imagecode.txt

The lmi.py script generates an output file with same name as its input, but with lmi_ prefixed, e.g., from pos_clipcap_imagecode.txt, it generates lmi_pos_clipcap_imagecode.txt.

The tab-delimited fields pos/lemma, count in neural, LMI in neural, count in human, LMI in human.

For example, this is how the lmi_pos_clipcap_imagecode.txt file generated as described above looks like (note that due to the randomness of set objects the order you see might be different):

% cat lmi_pos_clipcap_imagecode.txt
PART	47	-93.26909221059368	974	351.3044839930577
NOUN	8303	1508.5012383528035	12371	-1307.7769083625303
ADJ	831	-654.2878118190135	4622	1120.6978753783824
PUNCT	2339	373.7011866670712	3614	-329.85471571317436
PRON	394	-414.752909808355	2977	844.0252924004546
AUX	338	-456.021858148204	3552	1125.6657542282815
ADP	5087	1197.8516256876171	6916	-992.5173346756785
NUM	60	-70.0692991474408	516	153.66884599035922
SCONJ	8	-18.152277782604116	223	83.08150377989097
VERB	2139	65.66472157603138	4055	-64.15682198453561
INTJ	0	-0.0	3	1.223426208842744
PROPN	100	-19.529120226031615	263	22.502262692976768
CCONJ	214	-190.17539495631002	1340	347.9257717196973
SYM	0	-0.0	16	6.524939780494634
ADV	403	-311.55019460606235	2204	528.7010137349567
DET	5846	1267.7823641669524	8207	-1067.2359097372869

So, by sorting in decreasing order by field 3 or 5, you can get the POSs or lemmas that are more typical of the neural or human corpus (note that this illustrated here is with POSs, since it's easier to fit them into the readme, but in the paper we only used the lemma analysis, as the POS distribution can be easily eyeballed as a whole in the frequency spectrum (figure 2 in the paper).
